{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing models.\n",
      "Calculating pre-blending values.\n",
      "3\n",
      "\n",
      "Training classifier [0]ExtraTreesRegressor(n_estimators=60)\n",
      "Fold [0] norm. Gini = 0.30579, MSE = 2.86946\n",
      "Fold [1] norm. Gini = 0.31658, MSE = 2.91228\n",
      "Fold [2] norm. Gini = 0.28711, MSE = 2.84914\n",
      "Fold [3] norm. Gini = 0.30701, MSE = 2.87039\n",
      "Fold [4] norm. Gini = 0.30055, MSE = 2.85780\n",
      "Fold [5] norm. Gini = 0.29951, MSE = 2.84914\n",
      "Fold [6] norm. Gini = 0.25728, MSE = 2.84931\n",
      "Fold [7] norm. Gini = 0.31279, MSE = 2.83870\n",
      "Fold [8] norm. Gini = 0.30833, MSE = 2.80206\n",
      "Fold [9] norm. Gini = 0.29395, MSE = 2.83891\n",
      "Clf_0 Mean norm. Gini = 0.29889 (0.01614)\n",
      "\n",
      "Training classifier [1]RandomForestRegressor(n_estimators=30)\n",
      "Fold [0] norm. Gini = 0.29671, MSE = 2.84615\n",
      "Fold [1] norm. Gini = 0.34640, MSE = 2.87594\n",
      "Fold [2] norm. Gini = 0.29814, MSE = 2.84095\n",
      "Fold [3] norm. Gini = 0.33134, MSE = 2.81538\n",
      "Fold [4] norm. Gini = 0.32509, MSE = 2.83983\n",
      "Fold [5] norm. Gini = 0.30399, MSE = 2.82035\n",
      "Fold [6] norm. Gini = 0.30648, MSE = 2.80378\n",
      "Fold [7] norm. Gini = 0.31758, MSE = 2.83788\n",
      "Fold [8] norm. Gini = 0.32317, MSE = 2.77599\n",
      "Fold [9] norm. Gini = 0.31294, MSE = 2.81426\n",
      "Clf_1 Mean norm. Gini = 0.31618 (0.01494)\n",
      "\n",
      "Training classifier [2]GradientBoostingRegressor(n_estimators=30)\n",
      "Fold [0] norm. Gini = 0.32264, MSE = 2.75944\n",
      "Fold [1] norm. Gini = 0.38323, MSE = 2.80416\n",
      "Fold [2] norm. Gini = 0.30068, MSE = 2.76537\n",
      "Fold [3] norm. Gini = 0.32335, MSE = 2.75739\n",
      "Fold [4] norm. Gini = 0.34336, MSE = 2.77109\n",
      "Fold [5] norm. Gini = 0.32617, MSE = 2.74311\n",
      "Fold [6] norm. Gini = 0.32967, MSE = 2.71997\n",
      "Fold [7] norm. Gini = 0.34509, MSE = 2.77397\n",
      "Fold [8] norm. Gini = 0.34070, MSE = 2.69454\n",
      "Fold [9] norm. Gini = 0.32906, MSE = 2.75093\n",
      "Clf_2 Mean norm. Gini = 0.33440 (0.02033)\n",
      "Time taken for pre-blending calculations:  0:20:41.141740\n",
      "CV-Results [[0.30579107 0.31658205 0.28711344 0.30701194 0.30055259 0.29951447\n",
      "  0.25728146 0.31278913 0.30833004 0.29395071]\n",
      " [0.29670795 0.34639826 0.29814475 0.33134127 0.3250855  0.30399009\n",
      "  0.30647625 0.31758061 0.32317175 0.31294241]\n",
      " [0.32264139 0.38322703 0.30068336 0.32335073 0.3433561  0.32617059\n",
      "  0.32967143 0.34509118 0.34070462 0.3290566 ]]\n",
      "Blending models.\n",
      "Ridge Best alpha =  0.01\n",
      "Avg. CV-Score = 0.31649029246853555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from datetime import datetime\n",
    "                                 \n",
    "# Source of good version: https://www.kaggle.com/c/ClaimPredictionChallenge/forums/t/703/code-to-calculate-normalizedgini    \n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "     assert( len(actual) == len(pred) )\n",
    "     all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "     all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "     totalLosses = all[:,0].sum()\n",
    "     giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "     giniSum -= (len(actual) + 1) / 2.\n",
    "     return giniSum / len(actual)\n",
    " \n",
    "def normalized_gini(a, p):\n",
    "     return gini(a, p) / gini(a, a)\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    # load train data\n",
    "    train    = pd.read_csv('PropertyInspection_train.csv')\n",
    "    test     = pd.read_csv('PropertyInspection_test.csv')\n",
    "    \n",
    "    labels   = train.Hazard   \n",
    "    \n",
    "    test_ind = test.iloc[:,0]\n",
    "    train.drop('Hazard', axis=1, inplace=True)\n",
    "    train_ind = train.iloc[:,0]\n",
    "    train.drop('Id', axis=1, inplace=True)\n",
    "    test.drop('Id', axis=1, inplace=True)\n",
    "    Dropcols = ['T2_V10', 'T2_V7','T1_V13', 'T1_V10']\n",
    "    train.drop( Dropcols, axis = 1, inplace=True )\n",
    "    test.drop( Dropcols, axis = 1, inplace=True )\n",
    "    \n",
    "    catCols=['T1_V4', 'T1_V5','T1_V6', 'T1_V7', 'T1_V8', 'T1_V9', 'T1_V11', 'T1_V12', 'T1_V15',\n",
    "     'T1_V16', 'T1_V17', 'T2_V3', 'T2_V5', 'T2_V11', 'T2_V12', 'T2_V13']\n",
    "  \n",
    "    trainNumX=train.drop(catCols, axis=1)\n",
    "    trainCatVecX = pd.get_dummies(train[catCols])   \n",
    "    trainX = np.hstack((trainCatVecX,trainNumX))\n",
    "\n",
    "    testNumX=test.drop(catCols, axis=1)\n",
    "    testCatVecX = pd.get_dummies(test[catCols])  \n",
    "    testX = np.hstack((testCatVecX,testNumX))\n",
    " \n",
    "    return trainX, labels, train_ind, testX, test_ind\n",
    "     \n",
    "if __name__ == '__main__':    \n",
    "    DEVELOP = False\n",
    "\n",
    "    SEED = 42   \n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    X, Y, idx, testX, testidx = prepare_data()\n",
    "\n",
    "    print (\"Preparing models.\")\n",
    "  \n",
    "    if (DEVELOP==True):\n",
    "        # The DEV SET will be used for all training and validation purposes\n",
    "        # The TEST SET will never be used for training, it is the unseen set.\n",
    "        dev_cutoff = int(round(len(Y) * 4/5))\n",
    "        X_dev = X[:dev_cutoff]\n",
    "        Y_dev = Y[:dev_cutoff]\n",
    "        X_test = X[dev_cutoff:]\n",
    "        Y_test = Y[dev_cutoff:]              \n",
    "    else:    # else submit    \n",
    "        X_dev = X\n",
    "        Y_dev = Y\n",
    "        X_test = testX\n",
    "        \n",
    "    n_trees = 30\n",
    "    n_folds = 10\n",
    "  \n",
    "    # Our level 0 classifiers\n",
    "    clfs = [\n",
    "        ExtraTreesRegressor(n_estimators = n_trees *2),\n",
    "        RandomForestRegressor(n_estimators = n_trees),\n",
    "        GradientBoostingRegressor(n_estimators = n_trees)\n",
    "    ]\n",
    "\n",
    "    # Ready for cross validation\n",
    "#     skf = KFold(n=X_dev.shape[0], n_folds=n_folds)\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42)\n",
    "    skf  = kf.split(X_dev)   \n",
    "    # Pre-allocate the data\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "\n",
    "    print (\"Calculating pre-blending values.\")\n",
    "    start_time = datetime.now()\n",
    "    print(len(clfs))\n",
    "    #print(len(skf))\n",
    "  \n",
    "    cv_results = np.zeros((len(clfs), kf.n_splits))  # Number of classifiers x Number of folds\n",
    "        \n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print ('\\nTraining classifier [%s]%s' % (j, clf))\n",
    "        blend_test_j = np.zeros((X_test.shape[0], kf.n_splits)) # Number of testing data x Number of folds , we will take the mean of the predictions later\n",
    "        i=0\n",
    "        for train_index, cv_index in kf.split(X_dev):\n",
    "            #print ('Fold [%s]' % (i))\n",
    "            \n",
    "            # This is the training and validation set\n",
    "            X_train = X_dev[train_index]\n",
    "            Y_train = Y_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "            \n",
    "            #print(\"fit\")\n",
    "            clf.fit(X_train, Y_train)\n",
    "            \n",
    "            #print(\"blend\")\n",
    "            # This output will be the basis for our blended classifier to train against,\n",
    "            # which is also the output of our classifiers\n",
    "            one_result = clf.predict(X_cv)\n",
    "            blend_train[cv_index, j] = one_result\n",
    "            score = normalized_gini(Y_cv, blend_train[cv_index, j])\n",
    "            cv_results[j,i] = score\n",
    "            score_mse = metrics.mean_absolute_error(Y_cv, one_result)    \n",
    "            print ('Fold [%s] norm. Gini = %0.5f, MSE = %0.5f' % (i, score, score_mse)) \n",
    "            blend_test_j[:, i] = clf.predict(X_test)    \n",
    "            i += 1\n",
    "        # Take the mean of the predictions of the cross validation set\n",
    "        blend_test[:, j] = blend_test_j.mean(1)      \n",
    "        print ('Clf_%d Mean norm. Gini = %0.5f (%0.5f)' % (j, cv_results[j,].mean(), cv_results[j,].std()))\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    time_taken = end_time - start_time\n",
    "    print (\"Time taken for pre-blending calculations: \", time_taken)\n",
    "\n",
    "    print (\"CV-Results\", cv_results)\n",
    "    \n",
    "    # Start blending!    \n",
    "    print (\"Blending models.\")\n",
    "\n",
    "    alphas = [0.0001, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "    \n",
    "    bclf = RidgeCV(alphas=alphas, normalize=True, cv=5)\n",
    "    bclf.fit(blend_train, Y_dev)       \n",
    "    print (\"Ridge Best alpha = \", bclf.alpha_)\n",
    "   \n",
    "    # Predict now\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "    \n",
    "    if (DEVELOP):\n",
    "            score1 = metrics.mean_squared_error(Y_test, Y_test_predict)\n",
    "            score = normalized_gini(Y_test, Y_test_predict)\n",
    "            print ('Ridge MSE = %s normalized Gini = %s' % (score1, score))\n",
    "    else: # Submit! and generate solution\n",
    "        score = cv_results.mean()      \n",
    "        print ('Avg. CV-Score = %s' % (score))\n",
    "        #generate solution\n",
    "        submission = pd.DataFrame({\"Id\": testidx, \"Hazard\": Y_test_predict})\n",
    "        submission = submission.set_index('Id')\n",
    "        submission.to_csv(\"bench_gen_stacking.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
